# Multi AI Agent

A multi-agent AI system built with FastAPI, Streamlit, LangChain, and Groq, featuring web search capabilities through Tavily API.

## ğŸš€ Features

- **Multiple AI Models**: Support for various Groq AI models
  - `llama-3.3-70b-versatile` - Advanced language model
  - `llama-3.1-8b-instant` - Fast and efficient model
- **Web Search Integration**: Powered by Tavily API for real-time information retrieval
- **Interactive UI**: Built with Streamlit for easy interaction
- **RESTful API**: FastAPI backend for programmatic access
- **Customizable System Prompts**: Define your AI agent's behavior
- **Structured Logging**: Comprehensive logging system for debugging and monitoring

## ğŸ—ï¸ Architecture

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ backend/        # FastAPI backend
â”‚   â”‚   â””â”€â”€ api.py      # API endpoints
â”‚   â”œâ”€â”€ common/         # Shared utilities
â”‚   â”‚   â”œâ”€â”€ logger.py   # Logging configuration
â”‚   â”‚   â””â”€â”€ custom_exception.py
â”‚   â”œâ”€â”€ config/         # Configuration
â”‚   â”‚   â””â”€â”€ settings.py # Environment variables
â”‚   â”œâ”€â”€ core/           # Core logic
â”‚   â”‚   â””â”€â”€ ai_agent.py # AI agent implementation
â”‚   â””â”€â”€ frontend/       # Streamlit UI
â”‚       â””â”€â”€ ui.py       # User interface
â”œâ”€â”€ logs/               # Application logs
â”œâ”€â”€ Dockerfile          # Docker configuration
â”œâ”€â”€ Jenkinsfile         # CI/CD pipeline
â””â”€â”€ requirements.txt    # Python dependencies
```

## ğŸ“‹ Prerequisites

- Python 3.11+
- Groq API Key ([Get it here](https://console.groq.com/keys))
- Tavily API Key ([Get it here](https://app.tavily.com/))

## ğŸ”§ Installation

### 1. Clone the repository

```bash
git clone https://github.com/tamaraporf/multi-ai-agent.git
cd multi-ai-agent
```

### 2. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

Create a `.env` file in the root directory:

```env
GROQ_API_KEY=your_groq_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here
```

## ğŸš€ Usage

### Running the Application

Start both backend and frontend services:

```bash
python app/main.py
```

This will launch:
- **Backend API**: http://127.0.0.1:9999
- **Frontend UI**: http://localhost:8501

### Using the Web Interface

1. Open your browser and navigate to `http://localhost:8501`
2. Define your AI agent with a custom system prompt
3. Select an AI model from the dropdown
4. (Optional) Enable web search for real-time information
5. Enter your query and click "Ask Agent"

### Using the API

#### Health Check

```bash
curl http://127.0.0.1:9999/
```

#### Chat Endpoint

```bash
curl -X POST http://127.0.0.1:9999/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "llama-3.3-70b-versatile",
    "system_prompt": "You are a helpful assistant",
    "messages": ["What is the weather like today?"],
    "allow_search": true
  }'
```

## ğŸ³ Docker Deployment

### Build the image

```bash
docker build -t multi-ai-agent .
```

### Run the container

```bash
docker run -p 9999:9999 -p 8501:8501 \
  -e GROQ_API_KEY=your_key \
  -e TAVILY_API_KEY=your_key \
  multi-ai-agent
```

## ğŸ“Š API Documentation

### POST `/chat`

Send a message to the AI agent and receive a response.

**Request Body:**

```json
{
  "model_name": "llama-3.3-70b-versatile",
  "system_prompt": "You are a helpful assistant",
  "messages": ["Your question here"],
  "allow_search": false
}
```

**Response:**

```json
{
  "response": "AI agent's response here"
}
```

**Status Codes:**
- `200`: Success
- `400`: Invalid model name
- `500`: Server error

## ğŸ› ï¸ Development

### Project Structure

- **Backend (`app/backend/`)**: FastAPI application handling API requests
- **Frontend (`app/frontend/`)**: Streamlit web interface
- **Core (`app/core/`)**: AI agent logic using LangChain and LangGraph
- **Common (`app/common/`)**: Shared utilities (logging, exceptions)
- **Config (`app/config/`)**: Configuration and environment settings

### Available Models

| Model | Description | Best For |
|-------|-------------|----------|
| `llama-3.3-70b-versatile` | Advanced reasoning | Complex tasks, long conversations |
| `llama-3.1-8b-instant` | Fast responses | Quick queries, simple tasks |

### Logging

Logs are stored in the `logs/` directory with daily rotation:
- Format: `log_YYYY-MM-DD.log`
- Level: INFO and above
- Console and file output

## ğŸ”’ Security Best Practices

- Never commit `.env` files to version control
- Rotate API keys regularly
- Use environment variables for sensitive data
- Keep dependencies updated

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is part of the LLMOps Udemy course.

## ğŸ™ Acknowledgments

- [LangChain](https://python.langchain.com/) - AI application framework
- [Groq](https://groq.com/) - Fast AI inference
- [Tavily](https://tavily.com/) - Search API
- [FastAPI](https://fastapi.tiangolo.com/) - Modern web framework
- [Streamlit](https://streamlit.io/) - Interactive UI framework

## ğŸ“ Support

For issues and questions:
- Open an issue on GitHub
- Check the [documentation](https://github.com/tamaraporf/multi-ai-agent/wiki)

---

**Built with â¤ï¸ for LLMOps learning**
